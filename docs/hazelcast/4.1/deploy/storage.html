<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Untitled :: Hazelcast Documentation</title>
    <link rel="canonical" href="https://JakeSCahill.github.io/hazelcast/4.1/deploy/storage.html">
    <meta name="generator" content="Antora 2.3.4">
    <link rel="stylesheet" href="../../../_/css/site.css">
<link rel="stylesheet" href="../../../_/css/search.css">
    <script async src="https://www.googletagmanager.com/gtag/js?id=GTM-M267KFN"></script>
    <script>function gtag(){dataLayer.push(arguments)};window.dataLayer=window.dataLayer||[];gtag('js',new Date());gtag('config','GTM-M267KFN')</script>
  </head>
  <body class="article">
<header class="header" role="banner">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://JakeSCahill.github.io">Hazelcast Documentation</a>
        <div class="navbar-item">
          <input id="search-input" type="text" placeholder="Search docs">
        </div>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="#">Home</a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Products</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Hazelcast IMDG</a>
            <a class="navbar-item" href="#">Hazelcast Jet</a>
            <a class="navbar-item" href="#">Hazelcast Cloud</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Use cases</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Service A</a>
            <a class="navbar-item" href="#">Service B</a>
            <a class="navbar-item" href="#">Service C</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Resources</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Demos</a>
            <a class="navbar-item" href="#">GitHub</a>
            <a class="navbar-item" href="#">Community</a>
            <a class="navbar-item" href="#">Blog</a>
          </div>
        </div>
        <div class="navbar-item">
          <span class="control">
            <a class="button is-primary" href="#">Download</a>
          </span>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="hazelcast" data-version="4.1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../hazelcast_overview.html">Hazelcast IMDG</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../hazelcast_overview.html">Overview</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../get-started/getting_started.html">Get Started</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../get-started/glossary.html">Glossary</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../develop/hazelcast_clients.html">Develop Solutions</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../develop/jcache.html">JCache</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../develop/performance.html">Performance</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../develop/serialization.html">Serialization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../develop/striim_cdc.html">Striim Hot Cache</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../develop/transactions.html">Transactions</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../develop/distributed_query.html">Distributed Query</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../develop/distributed_sql.html">Distributed SQL</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../develop/distributed_events.html">Distributed Events</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../develop/distributed_computing.html">Distributed Computing</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="installing_upgrading.html">Deploy Clusters</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="starting_members_clients.html">Starting Members and Clients</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="setting_up_clusters.html">Setting Up Clusters</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="management.html">Management</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="security.html">Security</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="simulator.html">Hazelcast Simulator</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="wan.html">WAN Replication</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="network_partitioning.html">Network Partitioning</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../migrate/migration_guides.html">Migrate</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../troubleshoot/common_exception_types.html">Troubleshooting</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../reference/faq.html">Reference</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../reference/dds.html">Distributed data structures</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../reference/system_properties.html">System properties</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../reference/understanding_configuration.html">Configuration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../reference/phone_homes.html">Phone homes</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../reference/hazelcast_plugins.html">Plugins</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../contribute/extending_hazelcast.html">Contribute</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../contribute/licenses.html">Licenses</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../contribute/revision_history.html">Documentation changes</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Hazelcast IMDG</span>
    <span class="version">4.1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <span class="title">Hazelcast IMDG</span>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../hazelcast_overview.html">4.1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../preface.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
</nav>
  <div class="edit-this-page"><a class="git" href="https://github.com/JakeSCahill/docs-poc/edit/master/hazelcast-docs/modules/deploy/pages/storage.adoc">Edit this Page</a></div>
  </div>
  <div class="content">
<article class="doc">
<div class="sect1">
<h2 id="_storage"><a class="anchor" href="#_storage"></a>Storage</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This chapter describes Hazelcast&#8217;s High-Density Memory Store and
Hot Restart Persistence features along with their configurations and
gives recommendations on the storage sizing.</p>
</div>
<div class="sect2">
<h3 id="_high_density_memory_store"><a class="anchor" href="#_high_density_memory_store"></a>High-Density Memory Store</h3>
<div class="paragraph">
<p><strong class="navy">Hazelcast IMDG Enterprise HD</strong></p>
</div>
<div class="paragraph">
<p>By default, data structures in Hazelcast store data on heap in
serialized form for highest data compaction; yet, these data structures
are still subject to Java Garbage Collection (GC). Modern hardware has
much more available memory. If you want to make use of that hardware and
scale up by specifying higher heap sizes, GC becomes an increasing problem:
the application faces long GC pauses that make the application unresponsive.
Also, you may get out of memory errors if you fill your whole heap. Garbage
collection, which is the automatic process that manages the application&#8217;s
runtime memory, often forces you into configurations where multiple JVMs
with small heaps (sizes of 2-4GB per member) run on a single physical hardware
device to avoid garbage collection pauses. This results in oversized clusters
to hold the data and leads to performance level requirements.</p>
</div>
<div class="paragraph">
<p>In <strong class="navy">Hazelcast IMDG Enterprise HD</strong>, the High-Density Memory Store is
Hazelcast&#8217;s new enterprise in-memory storage solution. It solves garbage
collection limitations so that applications can exploit hardware memory
more efficiently without the need of oversized clusters. High-Density
Memory Store is designed as a pluggable memory manager which enables multiple
memory stores for different data structures. These memory stores are all
accessible by a common access layer that scales up to massive amounts of the main
memory on a single JVM by minimizing the GC pressure. High-Density Memory
Store enables predictable application scaling and boosts performance and
latency while minimizing garbage collection pauses.</p>
</div>
<div class="paragraph">
<p>This foundation includes, but is not limited to, storing keys and values
next to the heap in a native memory region.</p>
</div>
<div class="paragraph">
<p>High-Density Memory Store is currently provided for the following Hazelcast
features and implementations:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="#using-high-density-memory-store-with-map">Map</a></p>
</li>
<li>
<p><a href="#icache-configuration">JCache Implementation</a></p>
</li>
<li>
<p><a href="#near-cache">Near Cache</a></p>
</li>
<li>
<p><a href="#hot-restart-persistence">Hot Restart Persistence</a></p>
</li>
<li>
<p><a href="#using-high-density-memory-store-with-java-client">Java Client</a>, when using the Near Cache for client</p>
</li>
<li>
<p><a href="https://github.com/hazelcast/hazelcast-wm#using-high-density-memory-store" target="_blank" rel="noopener">Web Session Replications</a></p>
</li>
<li>
<p><a href="https://github.com/hazelcast/hazelcast-hibernate" target="_blank" rel="noopener">Hibernate 2nd Level Caching</a></p>
</li>
<li>
<p>Paging and Partition Predicates</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_configuring_high_density_memory_store"><a class="anchor" href="#_configuring_high_density_memory_store"></a>Configuring High-Density Memory Store</h4>
<div class="paragraph">
<p>To use the High-Density memory storage, the native memory usage
must be enabled using the programmatic or declarative configuration.
Also, you can configure its size, memory allocator type, minimum
block size, page size and metadata space percentage.</p>
</div>
<div class="paragraph">
<p>The following are the configuration element descriptions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>size:</strong> Size of the total native memory to allocate in megabytes.
Its default value is <strong>512 MB</strong>.</p>
</li>
<li>
<p><strong>allocator type</strong>: Type of the memory allocator. Available values are as follows:</p>
<div class="ulist">
<ul>
<li>
<p><strong>STANDARD</strong>: This option is used internally by Hazelcast&#8217;s
POOLED allocator type or for debugging/testing purposes.</p>
<div class="ulist">
<ul>
<li>
<p>With this option, the memory is allocated or deallocated using
your operating system&#8217;s default memory manager.</p>
</li>
<li>
<p>It uses GNU C Library&#8217;s standard <code>malloc()</code> and <code>free()</code> methods
which are subject to contention on multithreaded/multicore systems.</p>
</li>
<li>
<p>Memory operations may become slower when you perform a lot of small
allocations and deallocations.</p>
</li>
<li>
<p>It may cause large memory fragmentations, unless you use a method in
the background that emphasizes fragmentation avoidance, such as <code>jemalloc()</code>.
Note that a large memory fragmentation can trigger the Linux Out of Memory
Killer if there is no swap space enabled in your system. Even if the swap
space is enabled, the killer can be again triggered if there is not enough
swap space left.</p>
</li>
<li>
<p>If you still want to use the operating system&#8217;s default memory management,
you can set the allocator type to STANDARD in your native memory configuration.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>POOLED</strong>: This is the default option, Hazelcast&#8217;s own pooling memory allocator.</p>
<div class="ulist">
<ul>
<li>
<p>With this option, memory blocks are managed using internal memory pools.</p>
</li>
<li>
<p>It allocates memory blocks, each of which has a 4MB page size by default,
and splits them into chunks or merges them to create larger chunks when required.
Sizing of these chunks follows the <a href="https://en.wikipedia.org/wiki/Buddy_memory_allocation" target="_blank" rel="noopener">buddy memory allocation</a>
algorithm, i.e., power-of-two sizing.</p>
</li>
<li>
<p>It never frees memory blocks back to the operating system.
It marks disposed memory blocks as available to be used later,
meaning that these blocks are reusable.</p>
</li>
<li>
<p>Memory allocation and deallocation operations (except the ones
requiring larger sizes than the page size) do not interact with the
operating system mostly.</p>
</li>
<li>
<p>For memory allocation, it tries to find the requested memory size inside
the internal memory pools. If it cannot be found, then it interacts with the operating system.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p><strong>minimum block size:</strong> Minimum size of the blocks in bytes to split and
fragment a page block to assign to an allocation request. It is used only
by the <strong>POOLED</strong> memory allocator. Its default value is <strong>16 bytes</strong>.</p>
</li>
<li>
<p><strong>page size:</strong> Size of the page in bytes to allocate memory as a block.
It is used only by the <strong>POOLED</strong> memory allocator. Its default value is <code>1 &lt;&lt; 22</code> = <strong>4194304 Bytes</strong>, about <strong>4 MB</strong>.</p>
</li>
<li>
<p><strong>metadata space percentage:</strong> Defines the percentage of the allocated
native memory that is used for internal memory structures by the High-Density
Memory for tracking the used and available memory blocks. It is used only by
the <strong>POOLED</strong> memory allocator. Its default value is <strong>12.5</strong>. Please note that
when the memory runs out, you get a <code>NativeOutOfMemoryException</code>;  if your store
has a large number of entries, you should consider increasing this percentage.</p>
</li>
<li>
<p><strong>persistent-memory:</strong> See the <a href="#using-persistent-memory">Using Persistent Memory section</a> below.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The following is the programmatic configuration example.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">        MemorySize memorySize = new MemorySize(512, MemoryUnit.MEGABYTES);
        NativeMemoryConfig nativeMemoryConfig =
                new NativeMemoryConfig()
                        .setAllocatorType(NativeMemoryConfig.MemoryAllocatorType.POOLED)
                        .setSize(memorySize)
                        .setEnabled(true)
                        .setMinBlockSize(16)
                        .setPageSize(1 &lt;&lt; 20);</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following is the declarative configuration example.</p>
</div>
<div class="listingblock primary">
<div class="title">XML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-xml hljs" data-lang="xml">&lt;hazelcast&gt;
    ...
    &lt;native-memory allocator-type="POOLED" enabled="true"&gt;
        &lt;size unit="MEGABYTES" value="512"/&gt;
        &lt;min-block-size&gt;16&lt;/min-block-size&gt;
        &lt;page-size&gt;4194304&lt;/page-size&gt;
        &lt;metadata-space-percentage&gt;12.5&lt;/metadata-space-percentage&gt;
        &lt;persistent-memory&gt;
            &lt;directories&gt;
                &lt;directory numa-node="0"&gt;/mnt/pmem0&lt;/directory&gt;
                &lt;directory numa-node="1"&gt;/mnt/pmem1&lt;/directory&gt;
            &lt;/directories&gt;
        &lt;/persistent-memory&gt;
    &lt;/native-memory&gt;
    ...
&lt;/hazelcast&gt;</code></pre>
</div>
</div>
<div class="listingblock secondary">
<div class="title">YAML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yml hljs" data-lang="yml">hazelcast:
  native-memory:
    enabled: true
    allocator-type: POOLED
    size:
      unit: MEGABYTES
      value: 512
    min-block-size: 16
    page-size: 4194304
    metadata-space-percentage: 12.5
    persistent-memory:
        directories:
            - directory: /mnt/pmem0
              numa-node: 0
            - directory: /mnt/pmem1
              numa-node: 1</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You can check whether there is enough free physical memory for the
requested number of bytes using the system property <code>hazelcast.hidensity.check.freememory</code>.
See the <a href="#system-properties">System Properties appendix</a> on how to use Hazelcast
system properties.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="using-persistent-memory"><a class="anchor" href="#using-persistent-memory"></a>Using Persistent Memory</h4>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The High-Density Memory Store uses the persistent memory in its volatile mode,
which means all data is lost after the instance restarts. For durability, please check
the <a href="#hot-restart-persistence">Hot Restart Persistence</a> feature.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To support larger and more affordable storage for data structures like IMap,
ICache and Near Cache, Hazelcast provides integration with persistent memory
technologies like Intel&#174; Optane&#8482; DC. To benefit from the technology you
do not need to make any changes in your application code.
Only a few configuration changes are required.</p>
</div>
<div class="paragraph">
<p>The optional <code>persistent-memory</code> element under the <code>native-memory</code>
configuration block defines the directories where the persistent memory is mounted.
This option enables usage of the persistent memory on the cluster member so
that all data structures backed by High-Density Memory Store use the specified
mounting directories to store data. If no persistent memory directory is configured,
standard RAM is used.</p>
</div>
<div class="paragraph">
<p>The following snippets demonstrate how to configure the persistent memory as
High-Density Memory Store in Hazelcast. The example assumes dual-socket machine,
both sockets are populated with Intel&#174; Optane&#8482; DC persistent memory DIMMs that
are configured in interleaved mode. The two sockets' DIMMs are mounted as <code>/mnt/pmem0</code>
and <code>/mnt/pmem1</code> and are known as NUMA node0 and node1 respectively.</p>
</div>
<div class="paragraph">
<p><strong>Declarative Configuration:</strong></p>
</div>
<div class="listingblock primary">
<div class="title">XML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-xml hljs" data-lang="xml">&lt;hazelcast&gt;
    ...
    &lt;native-memory allocator-type="POOLED" enabled="true"&gt;
        &lt;size unit="GIGABYTES" value="100" /&gt;
        &lt;persistent-memory&gt;
            &lt;directories&gt;
                &lt;directory numa-node="0"&gt;/mnt/pmem0&lt;/directory&gt;
                &lt;directory numa-node="1"&gt;/mnt/pmem1&lt;/directory&gt;
            &lt;/directories&gt;
        &lt;/persistent-memory&gt;
    &lt;/native-memory&gt;
    ...
&lt;/hazelcast&gt;</code></pre>
</div>
</div>
<div class="listingblock secondary">
<div class="title">YAML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yml hljs" data-lang="yml">hazelcast:
  native-memory:
    enabled: true
    allocator-type: POOLED
    size:
      unit: GIGABYTES
      value: 100
    persistent-memory:
      directories:
        - directory: /mnt/pmem0
          numa-node: 0
        - directory: /mnt/pmem1
          numa-node: 1</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Programmatic Configuration:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">Config config = new Config();
NativeMemoryConfig memoryConfig = new NativeMemoryConfig()
                .setEnabled(true)
                .setSize(new MemorySize(100, MemoryUnit.GIGABYTES))
                .setAllocatorType(POOLED);

PersistentMemoryConfig pmemConfig = memoryConfig.getPersistentMemoryConfig()
                .addDirectoryConfig(new PersistentMemoryDirectoryConfig("/mnt/pmem0", 0))
                .addDirectoryConfig(new PersistentMemoryDirectoryConfig("/mnt/pmem1", 1));

config.setNativeMemoryConfig(memoryConfig);</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Note that integration with Intel&#174; Optane&#8482; DC is supported on Linux
operating system and it is for Optane DIMMs (not SSDs).
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="_allocation_strategies"><a class="anchor" href="#_allocation_strategies"></a>Allocation Strategies</h5>
<div class="paragraph">
<p>Since on multi-socket machines there could be multiple persistent memory
mount points, the memory allocations need to follow an allocation strategy.
Starting with 4.1, Hazelcast supports two allocation strategies:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Round-robin allocation strategy</p>
</li>
<li>
<p>NUMA-aware allocation strategy</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Hazelcast&#8217;s memory allocator chooses and statically caches one of them for
every allocator thread for the entire lifetime of the Hazelcast instance.</p>
</div>
<div class="sect5">
<h6 id="_round_robin_allocation_strategy"><a class="anchor" href="#_round_robin_allocation_strategy"></a>Round-robin Allocation Strategy</h6>
<div class="paragraph">
<p>Hazelcast iterates over the configured persistent memory directories and
makes sure every allocation is done in a different directory than the last.
This is a best-effort attempt to distribute the allocations evenly on the
persistent memory DIMMs, which is important from the utilization and
performance points of view as well. This is the default allocation strategy.</p>
</div>
</div>
<div class="sect5">
<h6 id="_numa_aware_allocation_strategy"><a class="anchor" href="#_numa_aware_allocation_strategy"></a>NUMA-aware Allocation Strategy</h6>
<div class="paragraph">
<p>The persistent memory modules are mounted in the memory slots just like the
regular memory modules and sharing the same memory bus. Therefore, the same
NUMA locality concerns apply to the persistent memory that apply to regular
memory. This means accessing the persistent memory modules attached to the
socket on which the current thread runs is cheaper than accessing the
persistent memory modules attached to a different socket. These are typically
referenced as NUMA-local and NUMA-remote memories. To achieve the best
possible performance, Hazelcast implements a NUMA-aware  allocation strategy
to ensure all persistent memory accesses are local, if certain conditions hold.</p>
</div>
<div class="paragraph">
<p>To enable this allocation strategy for a certain thread, the thread has to be
bounded to a single NUMA node, which means the kernel&#8217;s scheduler makes sure
the thread can be scheduled only on the CPUs of a single NUMA node. Starting
with Hazelcast 4.1 this can be done with thread group granularity. For the
detailed explanation please refer to the thread affinity documentation. What
makes the biggest impact on performance is enabling the NUMA-aware allocation
strategy for the operation threads. An example configuration for that is as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>-Dhazelcast.operation.thread.affinity=[0-9,20-29]:20,[10-19,30-39]:20</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above example configuration restricts all 40 operation threads to run on a single NUMA node
on a dual-socket 40 core system, where node0&#8217;s CPU set is <code>[0-9,20-29]</code> and
node1&#8217;s CPU set is <code>[10-19,30-39]</code>. The NUMA nodes and their CPU sets can be
discovered by the <code>numactl -H</code> command.</p>
</div>
<div class="paragraph">
<p>The second requirement for the NUMA-aware strategy is defining the NUMA node
for every persistent memory directory in the configuration. If both configurations
are done properly, the threads in the thread groups restricted to run on a single
NUMA node will use the NUMA-aware allocation strategy, while the rest of threads
will still use the round-robin strategy. To check which persistent memory is
attached to which NUMA node, the command <code>ndctl list -v -m fsdax</code> can be used.
Please check which mount point represents which persistent memory device in the
output of <code>ndctl</code>.</p>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_allocation_overflowing"><a class="anchor" href="#_allocation_overflowing"></a>Allocation Overflowing</h5>
<div class="paragraph">
<p>Since both allocation strategies try to allocate from a single persistent
memory directory, it may happen that the chosen directory cannot serve the
allocation request due to lack of free capacity. In this case, both strategies
take the other directories and try to serve the allocation from those. Please
note that this compromises the NUMA-aware strategy in the way that there will
be NUMA-remote persistent memory accesses.</p>
</div>
</div>
<div class="sect4">
<h5 id="_on_the_performance_of_persistent_memory"><a class="anchor" href="#_on_the_performance_of_persistent_memory"></a>On the Performance of Persistent Memory</h5>
<div class="paragraph">
<p>While the persistent memory modules are mounted next to the regular memory
modules and sharing the same memory bus, the two types of the modules have
different performance characteristics. First, the persistent memory modules
can be accessed with higher latency than the regular memory modules. Second,
while with the regular memory modules the performance of the reads and the
writes are not different, this is not the case with the persistent memory
modules. The persistent memory has an asymmetric performance profile, which
means the writes are slower than the reads.</p>
</div>
<div class="paragraph">
<p>Despite the above facts, whether the higher latency of the persistent memory
impacts the performance of Hazelcast depends on multiple factors. Since
Hazelcast is a distributed platform, the higher latency of the persistent memory
modules can easily be hidden by the latency variance of the network and in the
end, in certain use cases there may be no observable difference in the throughput
of Hazelcast if it stores its data on persistent memory or on regular memory.
Such a use case is caching, where accessing the entries remotely through
Hazelcast clients results in a very similar throughput. Based on our tests with
Intel&#174; Optane&#8482; DC persistent memory modules we recommend the Optane modules
for the caching use case up to 10KB entry size.</p>
</div>
<div class="paragraph">
<p>Other use cases that don&#8217;t involve networking, such as iterating over all entries
with entry processors can be impacted by the higher latency of the persistent
memory modules, especially, if the entry processors update a significant portion
of the entries. In general, in such a use case the higher the entry size, the
higher the impact on the performance. That means with smaller entry sizes the
performance of Hazelcast with persistent memory can be comparable to the
performance with regular memory.</p>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_sizing_practices"><a class="anchor" href="#_sizing_practices"></a>Sizing Practices</h3>
<div class="paragraph">
<p>Data in Hazelcast is both active data and backup data for high availability,
so the total memory footprint is the size of active data plus the size of
backup data. If you use a single backup, it means the total memory footprint
is two times the active data (active data + backup data). If you use, for example,
two backups, then the total memory footprint is three times the active data
(active data + backup data + backup data).</p>
</div>
<div class="paragraph">
<p>If you use only heap memory, each Hazelcast member with a 4 GB heap should
accommodate a maximum of 3.5 GB of total data (active and backup). If you use
the High-Density Memory Store, up to 75% of the configured physical memory
footprint may be used for active and backup data, with headroom of 25% for
normal memory fragmentation. In both cases, however, you should also keep some
memory headroom available to handle any member failure or explicit member shutdown.
When a member leaves the cluster, the data previously owned by the newly offline
member is distributed among the remaining members. For this reason, we recommend
that you plan to use only 60% of available memory, with 40% headroom to handle
member failure or shutdown.</p>
</div>
</div>
<div class="sect2">
<h3 id="_hot_restart_persistence"><a class="anchor" href="#_hot_restart_persistence"></a>Hot Restart Persistence</h3>
<div class="paragraph">
<p><strong class="navy">Hazelcast IMDG Enterprise HD</strong></p>
</div>
<div class="paragraph">
<p>This chapter explains Hazelcast&#8217;s Hot Restart Persistence feature. It provides fast
cluster restarts by storing the states of the cluster members on the disk.
This feature is currently provided for the Hazelcast map data structure and Hazelcast JCache implementation.</p>
</div>
<div class="sect3">
<h4 id="_hot_restart_persistence_overview"><a class="anchor" href="#_hot_restart_persistence_overview"></a>Hot Restart Persistence Overview</h4>
<div class="paragraph">
<p>Hot Restart Persistence enables you to get your cluster up and running swiftly
after a cluster restart. A restart can be caused by a planned shutdown
(including rolling upgrades) or a sudden cluster-wide crash, e.g., power outage.
For Hot Restart Persistence, required states for Hazelcast clusters and members are
introduced. See the <a href="#managing-cluster-and-member-states">Managing Cluster and Member States section</a>
for information on the cluster and member states. The purpose of the Hot Restart Persistence
is to provide a maintenance window for member operations and restart the cluster
in a fast way. It is not meant to recover the catastrophic shutdown of one member.</p>
</div>
<div class="paragraph">
<p>Hot Restart Persistence supports optional data encryption. See the <a href="#encryption-at-rest">Encryption at Rest section</a>
for more information.</p>
</div>
</div>
<div class="sect3">
<h4 id="_hot_restart_types"><a class="anchor" href="#_hot_restart_types"></a>Hot Restart Types</h4>
<div class="paragraph">
<p>The Hot Restart feature is supported for the following restart types:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Restart after a planned shutdown</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>The cluster is shut down completely and restarted with the exact same
previous setup and data.</p>
<div class="paragraph">
<p>You can shut down the cluster completely using the <code>HazelcastInstance.getCluster().shutdown()</code>
method or you can manually change the cluster state to <code>PASSIVE</code> and then shut
down each member one by one. When you send the command to shut the cluster down, i.e.,
<code>HazelcastInstance.getCluster().shutdown()</code>, the members that are not in the <code>PASSIVE</code>
state temporarily change their states to <code>PASSIVE</code>. Then, each member shuts itself down
by calling the method <code>HazelcastInstance.shutdown()</code>.</p>
</div>
<div class="paragraph">
<p>Difference between explicitly changing state to <code>PASSIVE</code> before shutdown and shutting
down cluster directly via <code>HazelcastInstance.getCluster().shutdown()</code> is, on the latter
case when cluster is restarted, the cluster state will be in the latest state before shutdown.
That means if cluster is <code>ACTIVE</code> before shutdown, cluster state automatically becomes
<code>ACTIVE</code> after restart is completed.</p>
</div>
</li>
<li>
<p>Rolling restart: The cluster is restarted intentionally member by member.
For example, this could be done to install an operating system patch or new hardware.</p>
<div class="paragraph">
<p>To be able to shut down the cluster member by member as part of a planned restart,
each member in the cluster should be in the <code>FROZEN</code> or <code>PASSIVE</code> state. After the
cluster state is changed to <code>FROZEN</code> or <code>PASSIVE</code>, you can manually shut down each
member by calling the method <code>HazelcastInstance.shutdown()</code>. When that member is
restarted, it rejoins the running cluster. After all members are restarted, the
cluster state can be changed back to <code>ACTIVE</code>.</p>
</div>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Restart after a cluster crash</strong>: The cluster is restarted after all its members
crashed at the same time due to a power outage, networking interruptions, etc.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_restart_process"><a class="anchor" href="#_restart_process"></a>Restart Process</h4>
<div class="paragraph">
<p>During the restart process, each member waits to load data until all the members in the
partition table are started. During this process, no operations are allowed. Once all
cluster members are started, Hazelcast changes the cluster state to <code>PASSIVE</code> and starts
to load data. When all data is loaded, Hazelcast changes the cluster state to its previous
known state before shutdown and starts to accept the operations which are allowed by the
restored cluster state.</p>
</div>
<div class="paragraph">
<p>If a member fails to either start, join the cluster in time (within the timeout), or
load its data, then that member is terminated immediately. After the problems causing
the failure are fixed, that member can be restarted. If the cluster start cannot be
completed in time, then all members fail to start. See the <a href="#configuring-hot-restart">Configuring Hot Restart section</a> for defining timeouts.</p>
</div>
<div class="paragraph">
<p>In the case of a restart after a cluster crash, the Hot Restart feature realizes that
it was not a clean shutdown and Hazelcast tries to restart the cluster with the last
saved data following the process explained above. In some cases, specifically when the
cluster crashes while it has an ongoing partition migration process, currently it is
not possible to restore the last saved state.</p>
</div>
<div class="sect4">
<h5 id="_restart_of_a_member_in_running_cluster"><a class="anchor" href="#_restart_of_a_member_in_running_cluster"></a>Restart of a Member in Running Cluster</h5>
<div class="paragraph">
<p>Assume the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>You have a cluster consisting of members A, B and C with Hot Restart enabled,
which is initially stable.</p>
</li>
<li>
<p>Member B is killed.</p>
</li>
<li>
<p>Member B restarts.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Since only a single member has failed, the cluster performed the standard
High Availability routine by recovering member B&#8217;s data from backups and
redistributing the data among the remaining members (the members A and C
in this case). Member B&#8217;s persisted Hot Restart data is completely irrelevant.</p>
</div>
<div class="paragraph">
<p>Furthermore, when a member starts with existing Hot Restart data,
it expects to find itself within a cluster that has been shut down
as a whole and is now restarting as a whole. Since the reality is that
the cluster has been running all along, member B&#8217;s persisted cluster state
does not match the actual state. Depending on the <a href="#configuring-hot-restart">automatic removal of stale data (<code>auto-remove-stale-data</code>)</a> configuration:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If <code>auto-remove-stale-data</code> is enabled, member B automatically deletes its
Hot Restart directory inside the <a href="#configuring-hot-restart">base directory</a>
(<code>base-dir</code>) and starts as a fresh, empty member. The cluster assigns some
partitions to it, unrelated to the partitions it owned before going down.</p>
</li>
<li>
<p>Otherwise, member B aborts the initialization and shuts down. To be able
to join the cluster, Hot Restart directory previously used by member B inside
the <a href="#configuring-hot-restart">base directory</a> (<code>base-dir</code>) must be deleted manually.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_force_start"><a class="anchor" href="#_force_start"></a>Force Start</h4>
<div class="paragraph">
<p>A member can crash permanently and then be unable to recover from the failure.
In that case, restart process cannot be completed since some of the members do
not start or fail to load their own data. In that case, you can force the cluster
to clean its persisted data and make a fresh start. This process is called <strong>force start</strong>.</p>
</div>
<div class="paragraph">
<p>Assume the following which is a valid scenario to use force start:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>You have a cluster consisting of members A and B which is initially stable.</p>
</li>
<li>
<p>Cluster transitions into <code>FROZEN</code> or <code>PASSIVE</code> state.</p>
</li>
<li>
<p>Cluster gracefully shuts down.</p>
</li>
<li>
<p>Member A restarts, but member B does not.</p>
</li>
<li>
<p>Member A uses its Hot Restart data to initiate the Hot Restart procedure.</p>
</li>
<li>
<p>Since it knows the cluster originally contained member B as well, it waits
for it to join.</p>
</li>
<li>
<p>This never happens.</p>
</li>
<li>
<p>Now you have the choice to Force Start the cluster without member B.</p>
</li>
<li>
<p>Cluster discards all Hot Restart data and starts empty.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can trigger the force start process using the Management Center,
REST API and cluster management scripts.</p>
</div>
<div class="paragraph">
<p>Please note that force start is a destructive process, which results
in deletion of persisted Hot Restart data.</p>
</div>
<div class="paragraph">
<p>See the <a href="https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#hot-restart" target="_blank" rel="noopener">Hot Restart functionality</a>
of the Management Center section to learn how you can perform a
force start using the Management Center.</p>
</div>
</div>
<div class="sect3">
<h4 id="_partial_start"><a class="anchor" href="#_partial_start"></a>Partial Start</h4>
<div class="paragraph">
<p>When one or more members fail to start or have incorrect Hot Restart
data (stale or corrupted data) or fail to load their Hot Restart data,
cluster becomes incomplete and restart mechanism cannot proceed.
One solution is to use <a href="#force-start">Force Start</a> and make a fresh
start with existing members. Another solution is to do a partial start.</p>
</div>
<div class="paragraph">
<p>Partial start means that the cluster starts with an incomplete member set.
Data belonging to those missing members is assumed lost and Hazelcast tries
to recover missing data using the restored backups. For example, if you have
minimum two backups configured for all maps and caches, then a partial start
up to two missing members will be safe against data loss. If there are more
than two missing members or there are maps/caches with less than two backups,
then data loss is expected.</p>
</div>
<div class="paragraph">
<p>Partial start is controlled by <code>cluster-data-recovery-policy</code> configuration
parameter and is not allowed by default. To enable partial start, one of the
configuration values <code>PARTIAL_RECOVERY_MOST_RECENT</code> or <code>PARTIAL_RECOVERY_MOST_COMPLETE</code>
should be set. See the <a href="#configuring-hot-restart">Configuring Hot Restart section</a>
for details.</p>
</div>
<div class="paragraph">
<p>When partial start is enabled, Hazelcast can perform a partial start
automatically or manually, in case of some members are unable to restart
successfully. Partial start proceeds automatically when some members fail to
start and join to the cluster in <code>validation-timeout-seconds</code>. After the
<code>validation-timeout-seconds</code> duration is passed, Hot Restart chooses to perform
partial start with the members present in the cluster. Moreover, partial start can
be requested manually using the
<a href="https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#hot-restart" target="_blank" rel="noopener">Management Center</a>,
<a href="#using-rest-api-for-cluster-management">REST API</a> and <a href="#example-usages-for-cluster-sh">cluster management scripts</a>
before the <code>validation-timeout-seconds</code> duration passes.</p>
</div>
<div class="paragraph">
<p>The other situation to decide to perform a partial start is failures during
the data load phase. When Hazelcast learns data load result of all members which
have passed the validation step, it automatically performs a partial start with
the ones which have successfully restored their Hot Restart data. Please note that
partial start does not expect every member to succeed in the data load step.
It completes the process when it learns data load result for every member and
there is at least one member which has successfully restored its Hot Restart data.
Relatedly, if it cannot learn data load result of all members before <code>data-load-timeout-seconds</code>
duration, it proceeds with the ones which have already completed the data load process.</p>
</div>
<div class="paragraph">
<p>Selection of members to perform partial start among live members is done
according to the <code>cluster-data-recovery-policy</code> configuration.
Set of members which are not selected by the <code>cluster-data-recovery-policy</code>
are called <code>Excluded members</code> and they are instructed to perform <a href="#force-start">force start</a>.
Excluded members are allowed to join cluster only when they clean their
Hot Restart data and make a fresh-new start. This is a completely automatic
process. For instance, if you start the missing members after partial start
is completed, they clean their Hot Restart data and join to the cluster.</p>
</div>
<div class="paragraph">
<p>Please note that partial start is a destructive process. Once it is completed,
it cannot be repeated with a new configuration. For this reason, one may need
to perform the partial start process manually. Automatic behavior of partial start
relies on <code>validation-timeout-seconds</code> and <code>data-load-timeout-seconds</code> configuration
values. If you need to control the process manually, <code>validation-timeout-seconds</code> and
<code>data-load-timeout-seconds</code> properties can be set to very big values so that
Hazelcast cannot make progress on timeouts automatically. Then, the overall
process can be managed manually via aforementioned methods, i.e.,
Management Center, REST API and cluster management scripts.</p>
</div>
</div>
<div class="sect3">
<h4 id="_configuring_hot_restart"><a class="anchor" href="#_configuring_hot_restart"></a>Configuring Hot Restart</h4>
<div class="paragraph">
<p>You can configure Hot Restart feature programmatically or declaratively.
There are two steps of configuration:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Enabling and configuring the Hot Restart feature globally in your Hazelcast
configuration: This is done using the configuration element <code>hot-restart-persistence</code>.
See the <a href="#global-hot-restart-configuration">Global Hot Restart Configuration section</a> below.</p>
</li>
<li>
<p>Enabling and configuring the Hazelcast data structures to use the
Hot Restart feature: This is done using the configuration element <code>hot-restart</code>.
See the <a href="#per-data-structure-hot-restart-configuration">Per Data Structure Hot Restart Configuration section</a> below.</p>
</li>
</ol>
</div>
<div class="sect4">
<h5 id="_global_hot_restart_configuration"><a class="anchor" href="#_global_hot_restart_configuration"></a>Global Hot Restart Configuration</h5>
<div class="paragraph">
<p>This is where you configure the Hot Restart feature itself using the
<code>hot-restart-persistence</code> element. The following are the descriptions of its attribute and sub-elements:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>enabled</code>: Attribute of the <code>hot-restart-persistence</code> element which
specifies whether the feature is globally enabled in your Hazelcast
configuration. Set this attribute to <code>true</code> if you want any of your
data structures to use the Hot Restart feature.</p>
</li>
<li>
<p><code>base-dir</code>: Specifies the parent directory where the Hot Restart data
is stored. The default value for <code>base-dir</code> is <code>hot-restart</code>. You can use
the default value, or you can specify the value of another folder containing
the Hot Restart configuration, but it is mandatory that <code>base-dir</code> element has
a value. This directory is created automatically if it does not exist.</p>
<div class="paragraph">
<p><code>base-dir</code> is used as the parent directory, and a unique Hot Restart
directory is created inside <code>base-dir</code> for each Hazelcast member which
uses the same <code>base-dir</code>. That means, <code>base-dir</code> can be shared among
multiple Hazelcast members safely. This is especially useful for cloud
environments where the members generally use a shared filesystem.</p>
</div>
<div class="paragraph">
<p>When a Hazelcast member starts, it tries to acquire the ownership of the
first available Hot Restart directory inside the <code>base-dir</code>. If <code>base-dir</code>
is empty or if the starting member fails to acquire the ownership of any
directory (happens when all the directories are already acquired by other
Hazelcast members), then it creates its own fresh directory.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Previously, <code>base-dir</code> was being used only by a single Hazelcast member.
If such an existing <code>base-dir</code> is configured for a Hazelcast member, Hot Restart
starts in legacy mode and <code>base-dir</code> is used only by a single member, without
creating a unique sub-directory. Other members trying to use that <code>base-dir</code>
fails during the startup.
</td>
</tr>
</table>
</div>
</li>
<li>
<p><code>backup-dir</code>: Specifies the directory under which Hot Restart snapshots
(Hot Backups) are stored. See the <a href="#hot-backup">Hot Backup section</a> for
more information.</p>
</li>
<li>
<p><code>parallelism</code>: Level of parallelism in Hot Restart Persistence. There
are this many I/O threads, each writing in parallel to its own files.
During the Hot Restart procedure, this many I/O threads are reading the files
and this many rebuilder threads are rebuilding the Hot Restart metadata. The
default value for this property is 1. This is a good default in most but not
all cases. You should measure the raw I/O throughput of your infrastructure and
test with different values of parallelism. In some cases such as dedicated
hardware higher parallelism can yield more throughput of Hot Restart. In other
cases such as running on EC2, it can yield diminishing returns - more thread
scheduling, more contention on I/O and less efficient garbage collection.</p>
</li>
<li>
<p><code>validation-timeout-seconds</code>: Validation timeout for the Hot Restart process
when validating the cluster members expected to join and the partition table on the whole cluster.</p>
</li>
<li>
<p><code>data-load-timeout-seconds</code>: Data load timeout for the Hot Restart process.
All members in the cluster should finish restoring their local data before this timeout.</p>
</li>
<li>
<p><code>cluster-data-recovery-policy</code>: Specifies the data recovery policy that is
respected during the Hot Restart cluster start. Valid values are;</p>
<div class="ulist">
<ul>
<li>
<p><code>FULL_RECOVERY_ONLY</code>: Starts the cluster only when all expected members
are present and correct. Otherwise, it fails. This is the default value.</p>
</li>
<li>
<p><code>PARTIAL_RECOVERY_MOST_RECENT</code>: Starts the cluster with the members which have
most up-to-date partition table and successfully restored their data. All other
members leave the cluster and force start themselves. If no member restores its
data successfully, cluster start fails.</p>
</li>
<li>
<p><code>PARTIAL_RECOVERY_MOST_COMPLETE</code>: Starts the cluster with the largest group of
members which have the same partition table version and successfully restored their
data. All other members leave the cluster and force start themselves. If no member
restores its data successfully, cluster start fails.</p>
</li>
</ul>
</div>
</li>
<li>
<p><code>auto-remove-stale-data</code>: Enables automatic removal of the stale Hot Restart data.
When a member terminates or crashes when the cluster state is <code>ACTIVE</code>, the remaining
members redistribute the data among themselves and the data persisted on terminated
member&#8217;s storage becomes stale. That terminated member cannot rejoin the cluster
without removing Hot Restart data. When auto-removal of stale Hot Restart data is enabled,
while restarting that member, Hot Restart data is automatically removed and it joins
the cluster as a completely new member. Otherwise, Hot Restart data should be removed manually.</p>
</li>
<li>
<p><code>encryption-at-rest</code>: Configures encryption on the Hot Restart data level.
See the <a href="#encryption-at-rest">Encryption at Rest section</a> for more information.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_per_data_structure_hot_restart_configuration"><a class="anchor" href="#_per_data_structure_hot_restart_configuration"></a>Per Data Structure Hot Restart Configuration</h5>
<div class="paragraph">
<p>This is where you configure the data structures of your choice, so that they can
have the Hot Restart feature. This is done using the <code>hot-restart</code> configuration
element. As it is explained in the <a href="#hot-restart-persistence">introduction</a>
paragraph, Hot Restart feature is currently supported by Hazelcast map data
structure and JCache implementation (<code>map</code> and <code>cache</code>), each of which has the
<code>hot-restart</code> configuration element. The following are the descriptions of this
element&#8217;s attribute and sub-element:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>enabled</code>: Attribute of the <code>hot-restart</code> element which specifies whether the
Hot Restart feature is enabled for the related data structure. Its default value
is <code>false</code>.</p>
</li>
<li>
<p><code>fsync</code>: Turning on <code>fsync</code> guarantees that data is persisted to the disk
device when a write operation returns successful response to the caller. By default,
<code>fsync</code> is turned off (<code>false</code>). That means data is persisted to the disk device
eventually, instead of on every disk write. This generally provides a better performance.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_hot_restart_configuration_examples"><a class="anchor" href="#_hot_restart_configuration_examples"></a>Hot Restart Configuration Examples</h5>
<div class="paragraph">
<p>The following are example configurations for a Hazelcast map and JCache implementation.</p>
</div>
<div class="paragraph">
<p><strong>Declarative Configuration</strong>:</p>
</div>
<div class="paragraph">
<p>An example configuration is shown below.</p>
</div>
<div class="listingblock primary">
<div class="title">XML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-xml hljs" data-lang="xml">&lt;hazelcast&gt;
    ...
    &lt;hot-restart-persistence enabled="true"&gt;
        &lt;base-dir&gt;/mnt/hot-restart&lt;/base-dir&gt;
        &lt;backup-dir&gt;/mnt/hot-backup&lt;/backup-dir&gt;
        &lt;validation-timeout-seconds&gt;120&lt;/validation-timeout-seconds&gt;
        &lt;data-load-timeout-seconds&gt;900&lt;/data-load-timeout-seconds&gt;
        &lt;cluster-data-recovery-policy&gt;FULL_RECOVERY_ONLY&lt;/cluster-data-recovery-policy&gt;
    &lt;/hot-restart-persistence&gt;
    ...
    &lt;map name="test-map"&gt;
        &lt;hot-restart enabled="true"&gt;
            &lt;fsync&gt;false&lt;/fsync&gt;
        &lt;/hot-restart&gt;
    &lt;/map&gt;
    ...
    &lt;cache name="test-cache"&gt;
        &lt;hot-restart enabled="true"&gt;
            &lt;fsync&gt;false&lt;/fsync&gt;
        &lt;/hot-restart&gt;
    &lt;/cache&gt;
    ...
&lt;/hazelcast&gt;</code></pre>
</div>
</div>
<div class="listingblock secondary">
<div class="title">YAML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yml hljs" data-lang="yml">hazelcast:
  hot-restart-persistence:
    enabled: true
    base-dir: /mnt/hot-restart
    backup-dir: /mnt/hot-backup
    validation-timeout-seconds: 120
    data-load-timeout-seconds: 900
    cluster-data-recovery-policy: FULL_RECOVERY_ONLY
  map:
    test-map:
      hot-restart:
        enabled: true
        fsync: false
  cache:
    test-cache:
      hot-restart:
        enabled: true
        fsync: false</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Programmatic Configuration</strong>:</p>
</div>
<div class="paragraph">
<p>The programmatic equivalent of the above declarative configuration is shown below.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">        Config config = new Config();
        HotRestartPersistenceConfig hotRestartPersistenceConfig = new HotRestartPersistenceConfig()
        .setEnabled(true)
        .setBaseDir(new File("/mnt/hot-restart"))
        .setParallelism(1)
        .setValidationTimeoutSeconds(120)
        .setDataLoadTimeoutSeconds(900)
        .setClusterDataRecoveryPolicy(HotRestartClusterDataRecoveryPolicy.FULL_RECOVERY_ONLY)
        .setAutoRemoveStaleData(true);
        config.setHotRestartPersistenceConfig(hotRestartPersistenceConfig);

        MapConfig mapConfig = config.getMapConfig("test-map");
        mapConfig.getHotRestartConfig().setEnabled(true);

        CacheSimpleConfig cacheConfig = config.getCacheConfig("test-cache");
        cacheConfig.getHotRestartConfig().setEnabled(true);</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_configuring_hot_restart_store_on_intel_optane_dc_persistent_memory"><a class="anchor" href="#_configuring_hot_restart_store_on_intel_optane_dc_persistent_memory"></a>Configuring Hot Restart Store on Intel Optane DC Persistent Memory</h5>
<div class="paragraph">
<p>Hazelcast can be configured to use Intel Optane DC Persistent Memory as
the Hot Restart directory. For this, you need to perform the following steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Configure the Persistent Memory as a File System</p>
</li>
<li>
<p>Configure the Hot Restart Store to Use Persistent Memory</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Using Persistent Memory, Hot Restart times can be drastically improved.
You can find the configuration steps in the Hot Restart Store section
of the <a href="https://hazelcast.com/resources/hazelcast-deployment-operations-guide/" target="_blank" rel="noopener">Hazelcast IMDG Operations and Deployment Guide</a>.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_movingcopying_hot_restart_data"><a class="anchor" href="#_movingcopying_hot_restart_data"></a>Moving/Copying Hot Restart Data</h4>
<div class="paragraph">
<p>After Hazelcast member owning the Hot Restart data is shutdown, Hot Restart
<code>base-dir</code> can be copied/moved to a different server (which may have different
IP address and/or different number of CPU cores) and Hazelcast member can be
restarted using the existing Hot Restart data on that new server. Having a new
IP address does not affect Hot Restart, since it does not rely on the IP address
of the server but instead uses <code>Member</code> UUID as a unique identifier.</p>
</div>
<div class="paragraph">
<p>This flexibility provides the following abilities:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Replacing one or more faulty servers with the new ones easily without
touching remaining cluster.</p>
</li>
<li>
<p>Using Hot Restart on the cloud environments easily. Sometimes cloud providers
do not preserve the IP addresses on restart or after shutdown. Also it is
possible to startup the whole cluster on a different set of machines.</p>
</li>
<li>
<p>Copying production data to the test environment, so that a more functional
test cluster can bet setup.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Unfortunately having different number of CPU cores is not that straightforward.
Hazelcast partition threads, by default, uses a heuristic from the number of
cores, e.g., <code># of partition threads = # of CPU cores</code>. When a Hazelcast member
is started on a server with a different CPU core count, number of Hazelcast
partition threads changes and that makes Hot Restart fail during the startup.
Solution is to explicitly set number of Hazelcast partition threads
(<code>hazelcast.operation.thread.count</code> system property) and Hot Restart <code>parallelism</code>
configuration and use the same parameters on the new server. For setting system
properties see the <a href="#system-properties">System Properties appendix</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="_hot_restart_persistence_design_details"><a class="anchor" href="#_hot_restart_persistence_design_details"></a>Hot Restart Persistence Design Details</h4>
<div class="paragraph">
<p>Hazelcast&#8217;s Hot Restart Persistence uses the log-structured
storage approach. The following is a top-level design description:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The only kind of update operation on persistent data is <em>appending</em>.</p>
</li>
<li>
<p>What is appended are facts about events that happened to the data model
represented by the store; either a new value was assigned to a key or a key was removed.</p>
</li>
<li>
<p>Each record associated with a key makes stale the previous record
that was associated with that key.</p>
</li>
<li>
<p>Stale records contribute to the amount of <em>garbage</em> present in
the persistent storage.</p>
</li>
<li>
<p>Measures are taken to remove garbage from the storage.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This kind of design focuses almost all of the system&#8217;s complexity
into the garbage collection (GC) process, stripping down the client&#8217;s
operation to the bare necessity of guaranteeing persistent behavior: a
simple file append operation. Consequently, the latency of operations is
close to the theoretical minimum in almost all cases. Complications arise
only during prolonged periods of maximum load; this is where the details of
the GC process begin to matter.</p>
</div>
</div>
<div class="sect3">
<h4 id="_concurrent_incremental_generational_gc"><a class="anchor" href="#_concurrent_incremental_generational_gc"></a>Concurrent, Incremental, Generational GC</h4>
<div class="paragraph">
<p>In order to maintain the lowest possible footprint in the update operation
latency, the following properties are built into the garbage collection process:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A dedicated thread performs the GC. In Hazelcast terms, this thread
is called the Collector and the application thread is called the Mutator.</p>
</li>
<li>
<p>On each update there is metadata to be maintained; this is done asynchronously
by the Collector thread. The Mutator enqueues update events to the Collector&#8217;s work queue.</p>
</li>
<li>
<p>The Collector keeps draining its work queue at all times, including the time
it goes through the GC cycle. Updates are taken into account at each stage in the
GC cycle, preventing the copying of already dead records into compacted files.</p>
</li>
<li>
<p>All GC-induced I/O competes for the same resources as the Mutator&#8217;s update operations.
Therefore, measures are taken to minimize the impact of I/O done during GC:</p>
<div class="ulist">
<ul>
<li>
<p>data is never read from files, but from RAM</p>
</li>
<li>
<p>a heuristic scheme is employed which minimizes the number of bytes written
to the disk for each kilobyte of the reclaimed garbage</p>
</li>
<li>
<p>measures are also taken to achieve a good interleaving of Collector and
Mutator operations, minimizing latency outliers perceived by the Mutator</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="_io_minimization_scheme"><a class="anchor" href="#_io_minimization_scheme"></a>I/O Minimization Scheme</h5>
<div class="paragraph">
<p>The success of this scheme is subject to a bet on the Weak Generational Garbage Hypothesis,
which states that a new record entering the system is likely to become garbage soon.
In other words, a key updated now is more likely than average to be updated again soon.</p>
</div>
<div class="paragraph">
<p>The scheme was taken from the seminal Sprite LFS paper,
<a href="http://www.cs.berkeley.edu/~brewer/cs262/LFS.pdf" target="_blank" rel="noopener">Rosenblum, Ousterhout, <em>The Design and Implementation of a Log-Structured File System</em></a>.
The following is an outline of the paper:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Data is not written to one huge file, but to many files of moderate size (8 MB) called "chunks".</p>
</li>
<li>
<p>Garbage is collected incrementally, i.e. by choosing several chunks, then
copying all their live data to new chunks, then deleting the old ones.</p>
</li>
<li>
<p>I/O is minimized using a collection technique which results in a bimodal
distribution of chunks with respect to their garbage content: most files
are either almost all live data or they are all garbage.</p>
</li>
<li>
<p>The technique consists of two main principles:</p>
<div class="ulist">
<ul>
<li>
<p>Chunks are selected based on their <em>Cost-Benefit factor</em> (see below).</p>
</li>
<li>
<p>Records are sorted by age before copying to new chunks.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_cost_benefit_factor"><a class="anchor" href="#_cost_benefit_factor"></a>Cost-Benefit Factor</h5>
<div class="paragraph">
<p>The Cost-Benefit factor of a chunk consists of two components multiplied together:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>The ratio of benefit (amount of garbage that can be collected) to
I/O cost (amount of live data to be written).</p>
</li>
<li>
<p>The age of the data in the chunk, measured as the age of the
youngest record it contains.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The essence is in the second component: given equal amount of garbage in all chunks,
it makes the young ones less attractive to the Collector.
Assuming the generational garbage hypothesis, this allows the young chunks to
quickly accumulate more garbage. On the flip side, it also ensures that even
files with little garbage are eventually garbage collected. This removes garbage
which would otherwise linger on, thinly spread across many chunk files.</p>
</div>
<div class="paragraph">
<p>Sorting records by age groups the young records together in a single chunk and
does the same for older records. Therefore the chunks are either tend to keep
their data live for a longer time, or quickly become full of garbage.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_hot_restart_performance_considerations"><a class="anchor" href="#_hot_restart_performance_considerations"></a>Hot Restart Performance Considerations</h4>
<div class="paragraph">
<p>In this section you can find performance test summaries which are results of
benchmark tests performed with a single Hazelcast member running on a physical server and on AWS R3.</p>
</div>
<div class="sect4">
<h5 id="_performance_on_a_physical_server"><a class="anchor" href="#_performance_on_a_physical_server"></a>Performance on a Physical Server</h5>
<div class="paragraph">
<p>We have tested a member which has an IMap with High-Density Data Store.
Its data size is changed for each test, started from 10 GB to 500 GB
(each map entry has a value of 1 KB).</p>
</div>
<div class="paragraph">
<p>The tests investigate the write and read performance of Hot Restart Persistence and
are performed on HP ProLiant servers with RHEL 7 operating system using Hazelcast Simulator.</p>
</div>
<div class="paragraph">
<p>The following are the specifications of the server hardware used for the test:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>CPU: 2x Intel&#174; Xeon&#174; CPU E5-2687W v3 @ 3.10GHz  with 10 cores per processor.
Total 20 cores, 40 with hyper threading enabled.</p>
</li>
<li>
<p>Memory: 768GB 2133 MHz memory 24x HP 32GB 4Rx4 PC4-2133P-L Kit</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The following are the storage media used for the test:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A hot-pluggable 2.5 inch HDD with 1 TB capacity and 10K RPM.</p>
</li>
<li>
<p>An SSD, Light Endurance PCle Workload Accelerator.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The below table shows the test results.</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="../_images/HotRestartPerf.png" alt="Hot Restart Perf"></span></p>
</div>
</div>
<div class="sect4">
<h5 id="_performance_on_aws_r3"><a class="anchor" href="#_performance_on_aws_r3"></a>Performance on AWS R3</h5>
<div class="paragraph">
<p>We have tested a member which has an IMap with High-Density Data Store:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>This map has 40 million distinct keys, each map entry is 1 KB.</p>
</li>
<li>
<p>High-Density Memory Store is 59 GiB whose 19% is metadata.</p>
</li>
<li>
<p>Hot Restart is configured with <code>fsync</code> turned off.</p>
</li>
<li>
<p>Data size reloaded on restart is 38 GB.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The tests investigate the write and read performance of Hot Restart Persistence
and are performed on R3.2xlarge and R3.4xlarge EC2 instances using Hazelcast Simulator.</p>
</div>
<div class="paragraph">
<p>The following are the AWS storage types used for the test:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Elastic Block Storage (EBS) General Purpose SSD (GP2)</p>
</li>
<li>
<p>Elastic Block Storage with Provisioned IOPS (IO1) (Provisioned 10,000 IOPS on a
340 GiB volume, enabled EBS-optimized on instance)</p>
</li>
<li>
<p>SSD-backed instance store</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The below table shows the test results.</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="../_images/HotRestartPerf2.png" alt="Hot Restart Perf2"></span></p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_hot_backup"><a class="anchor" href="#_hot_backup"></a>Hot Backup</h4>
<div class="paragraph">
<p>During Hot Restart operations you can take a snapshot of the Hot Restart Store
at a certain point in time. This is useful when you wish to bring up a new cluster
with the same data or parts of the data. The new cluster can then be used to share
load with the original cluster, to perform testing, QA or reproduce an issue on production data.</p>
</div>
<div class="paragraph">
<p>Simple file copying of a currently running cluster does not suffice and can produce
inconsistent snapshots with problems such as resurrection of deleted values or missing values.</p>
</div>
<div class="sect4">
<h5 id="_configuring_hot_backup"><a class="anchor" href="#_configuring_hot_backup"></a>Configuring Hot Backup</h5>
<div class="paragraph">
<p>To create snapshots you must first configure the Hot Restart backup directory.
You can configure the directory programmatically or declaratively using the following configuration element:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>backup-dir</code>: This element is included in the <code>hot-restart-persistence</code> and
denotes the destination under which backups are stored. If this element is not defined,
hot backup is disabled. If a directory is defined which does not exist, it is created on
the first backup. To avoid clashing data on multiple backups, each backup has a unique
sequence ID which determines the name of the directory which contains all Hot Restart data.
This unique directory is created as a subdirectory of the configured <code>backup-dir</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The following are the example configurations for Hot backup.</p>
</div>
<div class="paragraph">
<p><strong>Declarative Configuration</strong>:</p>
</div>
<div class="paragraph">
<p>An example configuration is shown below.</p>
</div>
<div class="listingblock primary">
<div class="title">XML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-xml hljs" data-lang="xml">&lt;hazelcast&gt;
    ...
    &lt;hot-restart-persistence enabled="true"&gt;
        &lt;backup-dir&gt;/mnt/hot-backup&lt;/backup-dir&gt;
	...
    &lt;/hot-restart-persistence&gt;
    ...
&lt;/hazelcast&gt;</code></pre>
</div>
</div>
<div class="listingblock secondary">
<div class="title">YAML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yml hljs" data-lang="yml">hazelcast:
  hot-restart-persistence:
    enabled: true
    backup-dir: /mnt/hot-backup</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Programmatic Configuration</strong>:</p>
</div>
<div class="paragraph">
<p>The programmatic equivalent of the above declarative configuration is shown below.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">HotRestartPersistenceConfig hotRestartPersistenceConfig = new HotRestartPersistenceConfig();
hotRestartPersistenceConfig.setBackupDir(new File("/mnt/hot-backup"));
...
config.setHotRestartPersistenceConfig(hotRestartPersistenceConfig);</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_using_hot_backup"><a class="anchor" href="#_using_hot_backup"></a>Using Hot Backup</h5>
<div class="paragraph">
<p>Once configured, you can initiate a new backup via API or from the Management Center.
The backup is started transactionally and cluster-wide. This means that either
all or none of the members start the same backup. The member which receives the backup
request determines a new backup sequence ID and send that information to all members.
If all members respond that no other backup is currently in progress and that
no other backup request has already been made, then the coordinating member commands
the other members to start the actual backup process. This creates a directory under
the configured <code>backup-dir</code> with the name <code>backup-&lt;backupSeq&gt;</code> and start copying the
data from the original store.</p>
</div>
<div class="paragraph">
<p>The backup process is initiated nearly instantaneously on all members. Note that
since there is no limitation as to when the backup process is initiated, it may be
initiated during membership changes, partition table changes or during normal data update.
Some of these operations may not be completed fully yet, which means that some members
will backup some data while some members will backup a previous version of the same data.
This is usually solved by the anti-entropy mechanism on the new cluster which
reconciles different versions of the same data. Please check the
<a href="#achieving-high-consistency-of-backup-data">Achieving High Consistency of Backup Data section</a>
for more information.</p>
</div>
<div class="paragraph">
<p>The duration of the backup process and the disk data usage drastically depends on
what is supported by the system and the configuration. Please check the
<a href="#achieving-high-performance-of-backup-process">Achieving high performance of backup process section</a>
for more information on achieving better resource usage of the backup process.</p>
</div>
<div class="paragraph">
<p>Following is an example of how to trigger the Hot Backup via API:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">HotRestartService service = instance.getCluster().getHotRestartService();
service.backup();</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>backupSeq</code> is generated by the hot backup process, but you can define
your own backup sequences as shown below:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">HotRestartService service = instance.getCluster().getHotRestartService();
long backupSeq = ...
service.backup(backupSeq);</code></pre>
</div>
</div>
<div class="paragraph">
<p>Keep in mind that the backup fails if any member contains a backup directory
with the name <code>backup-&lt;backupSeq&gt;</code>, where <code>backupSeq</code> is the given sequence.</p>
</div>
</div>
<div class="sect4">
<h5 id="_starting_the_cluster_from_a_hot_backup"><a class="anchor" href="#_starting_the_cluster_from_a_hot_backup"></a>Starting the Cluster From a Hot Backup</h5>
<div class="paragraph">
<p>As mentioned in the previous section, hot backup process creates subdirectories
named <code>backup-&lt;backupSeq&gt;</code> under the configured <a href="#configuring-hot-backup">hot backup directory</a>
(i.e., <code>backup-dir</code>). When starting your cluster with data from a hot backup, you need to set
the <a href="#global-hot-restart-configuration">base directory</a> (i.e., <code>base-dir</code>) to the desired backup subdirectory.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s say you have configured your hot backup directory as follows:</p>
</div>
<div class="listingblock primary">
<div class="title">XML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-xml hljs" data-lang="xml">&lt;hazelcast&gt;
    ...
    &lt;hot-restart-persistence enabled="true"&gt;
        &lt;backup-dir&gt;/mnt/hot-backup&lt;/backup-dir&gt;
	...
    &lt;/hot-restart-persistence&gt;
    ...
&lt;/hazelcast&gt;</code></pre>
</div>
</div>
<div class="listingblock secondary">
<div class="title">YAML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yml hljs" data-lang="yml">hazelcast:
  hot-restart-persistence:
    enabled: true
    backup-dir: /mnt/hot-backup</code></pre>
</div>
</div>
<div class="paragraph">
<p>And let&#8217;s say you have a subdirectory named <code>backup-2018Oct24</code> under the
backup directory <code>/mnt/hot-backup</code>. When you want to start your cluster with data
from this backup (<code>backup-2018Oct24</code>), here is the configuration you should have
for the <code>base-dir</code> while starting the cluster:</p>
</div>
<div class="listingblock primary">
<div class="title">XML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-xml hljs" data-lang="xml">&lt;hazelcast&gt;
    ...
    &lt;hot-restart-persistence enabled="true"&gt;
        &lt;base-dir&gt;backup-2018Oct24&lt;/base-dir&gt;
        &lt;parallelism&gt;1&lt;/parallelism&gt;
    &lt;/hot-restart-persistence&gt;
    ...
    &lt;map name="test-map"&gt;
        &lt;hot-restart enabled="true"&gt;
            &lt;fsync&gt;false&lt;/fsync&gt;
        &lt;/hot-restart&gt;
    &lt;/map&gt;
    ...
&lt;/hazelcast&gt;</code></pre>
</div>
</div>
<div class="listingblock secondary">
<div class="title">YAML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yml hljs" data-lang="yml">hazelcast:
  hot-restart-persistence:
    enabled: true
    base-dir: backup-2018Oct24
    parallelism: 1
  map:
    test-map:
      hot-restart:
        enabled: true
        fsync: false</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_achieving_high_consistency_of_backup_data"><a class="anchor" href="#_achieving_high_consistency_of_backup_data"></a>Achieving High Consistency of Backup Data</h5>
<div class="paragraph">
<p>The backup is initiated nearly simultaneously on all members but you can
encounter some inconsistencies in the data. This is because some members might have
and some might not have received updated values yet from executed operations,
because the system could be undergoing partition and membership changes or
because there are some transactions which have not yet been committed.</p>
</div>
<div class="paragraph">
<p>To achieve a high consistency of data on all members, the cluster should be
put to <code>PASSIVE</code> state for the duration of the call to the backup method.
See the <a href="#cluster-member-states">Cluster Member States section</a> on information on how to do this.
The cluster does not need to be in <code>PASSIVE</code> state for the entire
duration of the backup process, though. Because of the design, only partition metadata
is copied synchronously during the invocation of the backup method. Once the backup method has returned,
all cluster metadata is copied and the exact partition data which needs to be copied is marked.
After that, the backup process continues asynchronously and you can return the cluster to the
<code>ACTIVE</code> state and resume operations.</p>
</div>
</div>
<div class="sect4">
<h5 id="_achieving_high_performance_of_backup_process"><a class="anchor" href="#_achieving_high_performance_of_backup_process"></a>Achieving High Performance of Backup Process</h5>
<div class="paragraph">
<p>Because of the design of Hot Restart Store, we can use hard links to achieve
backups/snapshots of the store. The hot backup process uses hard links
whenever possible because they provide big performance benefits and because
the backups share disk usage.</p>
</div>
<div class="paragraph">
<p>The performance benefit comes from the fact that Hot Restart file contents are
not being duplicated (thus using disk and I/O resources) but rather a new file name
is created for the same contents on disk (another pointer to the same inode).
Since all backups and stores share the same inode, disk usage drops.</p>
</div>
<div class="paragraph">
<p>The bigger the percentage of stable data in the Hot Restart Store
(data not undergoing changes), the more files each backup shares with the operational
Hot Restart Store and the less disk space it uses. For the hot backup to use hard links,
you must be running Hazelcast members on JDK 7 or higher and must satisfy all requirements for the
<a href="https://docs.oracle.com/javase/7/docs/api/java/nio/file/Files.html#createLink(java.nio.file.Path,%20java.nio.file.Path)" target="_blank" rel="noopener">Files.createLink() method</a> to be supported.</p>
</div>
<div class="paragraph">
<p>The backup process initially attempts to create a new hard link and
if that fails for any reason it continues by copying the data.
Subsequent backups also attempt to use hard links.</p>
</div>
</div>
<div class="sect4">
<h5 id="_backup_process_progress_and_completion"><a class="anchor" href="#_backup_process_progress_and_completion"></a>Backup Process Progress and Completion</h5>
<div class="paragraph">
<p>Only cluster and distributed object metadata is copied synchronously
during the invocation of the backup method. The rest of the
Hot Restart Store containing partition data is copied asynchronously
after the method call has ended. You can track the progress by API or
view it from the Management Center.</p>
</div>
<div class="paragraph">
<p>An example of how to track the progress via API is shown below:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">HotRestartService service = instance.getCluster().getHotRestartService();
BackupTaskStatus status = service.getBackupTaskStatus();
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>The returned object contains the local member&#8217;s backup status:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>the backup state (NOT_STARTED, IN_PROGRESS, FAILURE, SUCCESS)</p>
</li>
<li>
<p>the completed count</p>
</li>
<li>
<p>the total count</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The completed and total count can provide you a way to track the
percentage of the copied data. Currently the count defines the
number of copied and total local member Hot Restart Stores
(defined by <code>HotRestartPersistenceConfig.setParallelism()</code>)
but this can change at a later point to provide greater resolution.</p>
</div>
<div class="paragraph">
<p>Besides tracking the Hot Restart status by API, you can view the status in the
Management Center and you can inspect the on-disk files for each member.
Each member creates an <code>inprogress</code> file which is created in each of the copied Hot Restart Stores.
This means that the backup is currently in progress. When the backup task completes
the backup operation, this file is removed. If an error occurs during the backup task,
the <code>inprogress</code> file is renamed to <code>failure</code> which contains a stack trace of the exception.</p>
</div>
</div>
<div class="sect4">
<h5 id="_backup_task_interruption_and_cancellation"><a class="anchor" href="#_backup_task_interruption_and_cancellation"></a>Backup Task Interruption and Cancellation</h5>
<div class="paragraph">
<p>Once the backup method call has returned and asynchronous copying of the
partition data has started, the backup task can be interrupted.
This is helpful in situations where the backup task has started at an inconvenient time.
For instance, the backup task could be automatized and it could be accidentally triggered
during high load on the Hazelcast instances, causing the performance of the Hazelcast instances to drop.</p>
</div>
<div class="paragraph">
<p>The backup task mainly uses disk IO, consumes little CPU and it generally
does not last for a long time (although you should test it with your environment
to determine the exact impact). Nevertheless, you can abort the backup tasks
on all members via a cluster-wide interrupt operation.
This operation can be triggered programmatically or from the Management Center.</p>
</div>
<div class="paragraph">
<p>An example of programmatic interruption is shown below:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">HotRestartService service = instance.getCluster().getHotRestartService();
service.interruptBackupTask();
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>This method sends an interrupt to all members.
The interrupt is ignored if the backup task is currently not in progress
so you can safely call this method even though it has previously been
called or when some members have already completed their local backup tasks.</p>
</div>
<div class="paragraph">
<p>You can also interrupt the local member backup task as shown below:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">HotRestartService service = instance.getCluster().getHotRestartService();
service.interruptLocalBackupTask();
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>The backup task stops as soon as possible and it does not remove the
disk contents of the backup directory meaning that you must remove it manually.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="encryption-at-rest"><a class="anchor" href="#encryption-at-rest"></a>Encryption at Rest</h4>
<div class="paragraph">
<p>Records stored in the Hot Restart Store may contain sensitive information. This sensitive
information may be present in the keys, in the values, or in both. In Hot Restart terms,
Encryption at Rest concerns with encryption on the chunk file level. Since complete chunk
files are encrypted, all data stored in the Hot Restart Store is protected when
Encryption at Rest is enabled.</p>
</div>
<div class="paragraph">
<p>Data persisted in the Hot Restart Store is encrypted using symmetric encryption. The
implementation is based on
<a href="https://docs.oracle.com/javase/8/docs/technotes/guides/security/crypto/CryptoSpec.html" target="_blank" rel="noopener">Java Cryptography Architecture</a> (JCA).
The encryption scheme uses two levels of encryption keys: auto-generated Hot Restart
Store-level encryption keys (one per configured parallelism) that are used to encrypt
the chunk files and a master encryption key that is used to encrypt the store-specific
encryption keys. The master encryption key is sourced from an external system called Secure
Store and, in contrast to the Hot Restart Store-level encryption keys, it is not persisted
anywhere within the Hot Restart Store.</p>
</div>
<div class="paragraph">
<p>When Hot Restart with Encryption at Rest is first enabled on a member, the member contacts
the Secure Store during the startup and retrieves the master encryption key. Then it generates
the Hot Restart Store-level encryption keys for the parallel Stores and stores them (encrypted
using the master key) under the Hot Restart Store&#8217;s directory. The subsequent writes to Hot
Restart chunk files will be encrypted using the Store-level encryption key. During Hot Restart,
the member retrieves the master encryption key from the Secure Store, decrypts the Store-level
encryption keys and uses those to decrypt the chunk files.</p>
</div>
<div class="paragraph">
<p>Master key rotation is supported. If the master encryption key changes in the Secure Store,
the Hot Restart subsystem will detect it and retrieve the new master encryption key. During
this process, it will also re-encrypt the Hot Restart Store-level encryption keys using
the new master encryption key.</p>
</div>
<div class="paragraph">
<p>The <a href="#configuring-secure-store">Configuring a Secure Store section</a> provides information
about the supported Secure Store types.</p>
</div>
<div class="sect4">
<h5 id="_configuring_encryption_at_rest"><a class="anchor" href="#_configuring_encryption_at_rest"></a>Configuring Encryption at Rest</h5>
<div class="paragraph">
<p>Encryption at Rest can be enabled and configured programmatically or declaratively using the
<code>encryption-at-rest</code> sub-element of <code>hot-restart-persistence</code>. The <code>encryption-at-rest</code>
element has the following attributes and sub-elements:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>enabled</code>: Attribute that specifies whether Encryption at Rest is enabled; <code>false</code>
by default.</p>
</li>
<li>
<p><code>algorithm</code>: Specifies the symmetric cipher to use (such as <code>AES/CBC/PKCS5Padding</code>).</p>
</li>
<li>
<p><code>salt</code>: The encryption salt.</p>
</li>
<li>
<p><code>key-size</code>: The size of the auto-generated Hot Restart Store-level encryption key.</p>
</li>
<li>
<p><code>secure-store</code>: Specifies the Secure Store to use for the retrieval of master
encryption keys. See the <a href="#configuring-secure-store">Configuring a Secure Store section</a>
for more details.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The following are the example configurations for Encryption at Rest.</p>
</div>
<div class="paragraph">
<p><strong>Declarative Configuration</strong>:</p>
</div>
<div class="paragraph">
<p>An example configuration is shown below.</p>
</div>
<div class="listingblock primary">
<div class="title">XML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-xml hljs" data-lang="xml">&lt;hazelcast&gt;
    ...
    &lt;hot-restart-persistence enabled="true"&gt;
        ...
        &lt;encryption-at-rest enabled="true"&gt;
            &lt;algorithm&gt;AEC/CBC/PKCS5Padding&lt;/algorithm&gt;
            &lt;salt&gt;thesalt&lt;/salt&gt;
            &lt;key-size&gt;128&lt;/key-size&gt;
            &lt;secure-store&gt;...&lt;/secure-store&gt;
        &lt;/encryption-at-rest&gt;
        ...
    &lt;/hot-restart-persistence&gt;
    ...
&lt;/hazelcast&gt;</code></pre>
</div>
</div>
<div class="listingblock secondary">
<div class="title">YAML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yml hljs" data-lang="yml">hazelcast:
  hot-restart-persistence:
    enabled: true
    encryption-at-rest:
      enabled: true
      algorithm: AES/CBC/PKCS5Padding
      salt: thesalt
      key-size: 128
      secure-store:
         ...</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Programmatic Configuration</strong>:</p>
</div>
<div class="paragraph">
<p>The programmatic equivalent of the above declarative configuration is shown below.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">        HotRestartPersistenceConfig hotRestartPersistenceConfig = new HotRestartPersistenceConfig();
        EncryptionAtRestConfig encryptionAtRestConfig =
                hotRestartPersistenceConfig.getEncryptionAtRestConfig();
        encryptionAtRestConfig.setEnabled(true)
                .setAlgorithm("AES/CBC/PKCS5Padding")
                .setSalt("thesalt")
                .setKeySize(128)
                .setSecureStoreConfig(secureStore());</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="configuring-secure-store"><a class="anchor" href="#configuring-secure-store"></a>Configuring a Secure Store</h5>
<div class="paragraph">
<p>A Secure Store represents a (secure) source of master encryption keys and
is required for using Encryption at Rest.</p>
</div>
<div class="paragraph">
<p>Hazelcast IMDG Enterprise provides Secure Store implementations for the
<a href="https://docs.oracle.com/javase/8/docs/api/java/security/KeyStore.html" target="_blank" rel="noopener">Java KeyStore</a>
and for <a href="https://www.vaultproject.io/" target="_blank" rel="noopener">HashiCorp Vault</a>.</p>
</div>
<div class="paragraph">
<p><strong>Java KeyStore Secure Store</strong></p>
</div>
<div class="paragraph">
<p>The Java KeyStore Secure Store provides integration with the <a href="https://docs.oracle.com/javase/8/docs/api/java/security/KeyStore.html" target="_blank" rel="noopener">Java KeyStore</a>.
It can be configured programmatically or declaratively using the
<code>keystore</code> sub-element of <code>secure-store</code>. The <code>keystore</code> element has the following sub-elements:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>path</code>: The path to the KeyStore file.</p>
</li>
<li>
<p><code>type</code>: The type of the KeyStore (<code>PKCS12</code>, <code>JCEKS</code>, etc.).</p>
</li>
<li>
<p><code>password</code>: The KeyStore password.</p>
</li>
<li>
<p><code>current-key-alias</code>: The alias for the current encryption key entry (optional).</p>
</li>
<li>
<p><code>polling-interval</code>: The polling interval (in seconds) for checking for changes in the KeyStore.
Disabled by default.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Sensitive configuration properties such as <code>password</code> should be protected using
<a href="#variable-replacers">encryption replacers</a>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The Java KeyStore Secure treats all <code>KeyStore.SecretKeyEntry</code> entries stored in the KeyStore as
encryption keys. It expects that these entries use the same protection password as the KeyStore
itself. Entries of other types (private key entries, certificate entries) are ignored. If
<code>current-key-alias</code> is set, the corresponding entry will be treated as the current encryption key;
otherwise, the highest entry in the alphabetical order will be used. The remaining entries will
represent historical versions of the encryption key.</p>
</div>
<div class="paragraph">
<p>An example declarative configuration is shown below:</p>
</div>
<div class="listingblock primary">
<div class="title">XML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-xml hljs" data-lang="xml">&lt;secure-store&gt;
    &lt;keystore&gt;
        &lt;path&gt;/path/to/keystore.file&lt;/path&gt;
        &lt;type&gt;PKCS12&lt;/type&gt;
        &lt;password&gt;password&lt;/password&gt;
        &lt;current-key-alias&gt;current&lt;/current-key-alias&gt;
        &lt;polling-interval&gt;60&lt;/polling-interval&gt;
    &lt;/keystore&gt;
&lt;/secure-store&gt;</code></pre>
</div>
</div>
<div class="listingblock secondary">
<div class="title">YAML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yml hljs" data-lang="yml">secure-store:
  keystore:
    path: /path/to/keystore.file
    type: PKCS12
    password: password
    current-key-alias: current
    polling-interval: 60</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following is an equivalent programmatic configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">        JavaKeyStoreSecureStoreConfig keyStoreConfig =
                new JavaKeyStoreSecureStoreConfig(new File("/path/to/keystore.file"))
                        .setType("PKCS12")
                        .setPassword("password")
                        .setCurrentKeyAlias("current")
                        .setPollingInterval(60);</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>HashiCorp Vault Secure Store</strong></p>
</div>
<div class="paragraph">
<p>The HashiCorp Vault Secure Store provides integration with <a href="https://www.vaultproject.io/" target="_blank" rel="noopener">HashiCorp Vault</a>.
It can be configured programmatically or declaratively using the
<code>vault</code> sub-element of <code>secure-store</code>. The <code>vault</code> element has the following sub-elements:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>address</code>: The address of the Vault server.</p>
</li>
<li>
<p><code>secret-path</code>: The secret path under which the encryption keys are stored.</p>
</li>
<li>
<p><code>token</code>: The Vault authentication token.</p>
</li>
<li>
<p><code>polling-interval</code>: The polling interval (in seconds) for checking for changes in Vault. Disabled
by default.</p>
</li>
<li>
<p><code>ssl</code>: The TLS/SSL configuration for HTTPS support. See the <a href="#tlsssl">TLS/SSL section</a> for more
information about how to use the <code>ssl</code> element.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Sensitive configuration properties such as <code>token</code> should be protected using
<a href="#variable-replacers">encryption replacers</a>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The HashiCorp Vault Secure Store implementation uses the official REST API to integrate with
HashiCorp Vault. Only for the <a href="https://www.vaultproject.io/docs/secrets/kv/index.html" target="_blank" rel="noopener">KV secrets engine</a>,
both KV V1 and KV V2 can be used, but since only V2 provides secrets versioning, this is
the recommended option. With KV V1 (no versioning support), only one version of the encryption
key can be kept, whereas with KV V2, the HashiCorp Vault Secure Store is able to retrieve
also the historical encryption keys. (Note that the size of the version history is configurable
on the Vault side.) Having access to the previous encryption keys may be critical to avoid
scenarios where the Hot Restart data becomes undecryptable because the master encryption key
is no longer usable (for instance, when the original master encryption key got rotated out
in the Secure Store while the cluster was down).</p>
</div>
<div class="paragraph">
<p>The encryption key is expected to be stored at the specified secret path and represented as a
single key/value pair in the following format:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>name=Base64-encoded-data</code></pre>
</div>
</div>
<div class="paragraph">
<p>where name can be an arbitrary string. Multiple key/value pairs under the same secret path are not
supported. Here is an example of how such a key/value pair can be stored using the HashiCorp
Vault command-line client (under the secret path <code>hz/cluster</code>):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>vault kv put hz/cluster value=HEzO124Vz...</code></pre>
</div>
</div>
<div class="paragraph">
<p>With KV V2, a second <code>put</code> to the same secret path creates a new version of the encryption key.
With KV V1, it simply overwrites the current encryption key, discarding the old value.</p>
</div>
<div class="paragraph">
<p>An example declarative configuration is shown below:</p>
</div>
<div class="listingblock primary">
<div class="title">XML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-xml hljs" data-lang="xml">&lt;secure-store&gt;
    &lt;vault&gt;
        &lt;address&gt;http://localhost:1234&lt;/address&gt;
        &lt;secret-path&gt;secret/path&lt;/secret-path&gt;
        &lt;token&gt;token&lt;/token&gt;
        &lt;polling-interval&gt;60&lt;/polling-interval&gt;
        &lt;ssl&gt;...&lt;/ssl&gt;
    &lt;/vault&gt;
&lt;/secure-store&gt;</code></pre>
</div>
</div>
<div class="listingblock secondary">
<div class="title">YAML</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yml hljs" data-lang="yml">secure-store:
  vault:
    address: http://localhost:1234
    secret-path: secret/path
    token: token
    polling-interval: 60
    ssl:
      ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following is an equivalent programmatic configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">        VaultSecureStoreConfig vaultConfig =
                new VaultSecureStoreConfig("http://localhost:1234", "secret/path", "token")
                        .setPollingInterval(60);
        configureSSL(vaultConfig.getSSLConfig());</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</article>
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
  </div>
</main>
</div>
<footer class="footer">
  <p>This page was built using the Antora default UI.</p>
  <p>The source code for this UI is licensed under the terms of the MPL-2.0 license.</p>
</footer>
<script src="../../../_/js/site.js"></script>
<link rel="shortcut icon" href="http://hazelcast.com/images/favicon-imdg.png">
<script src="../../../_/js/vendor/lunr.js"></script>
<script src="../../../_/js/vendor/search.js" id="search-script" data-base-path="../../.." data-page-path="/hazelcast/4.1/deploy/storage.html"></script>
<script async src="../../../_/../search-index.js"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
