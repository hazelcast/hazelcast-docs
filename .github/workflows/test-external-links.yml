name: Test external links

on:
  pull_request:
  workflow_dispatch:
  schedule:
    - cron: "0 12 * * 1" # Runs at 12:00, only on Monday
     
jobs:
  test-external-links:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'
  
      - name: Install Lynx
        run: |
          sudo apt-get update
          sudo apt-get install -y lynx
  
      - name: Build documentation
        run: |
          npm install
          npm run-script build
  
      - run: |
          echo "temp_file=$(mktemp)" >> $GITHUB_ENV
  
      - name: Extract links
        run: |
          ${RUNNER_DEBUG:+set -o xtrace}

          # Extract all unique URLs
          # Faster than potentially checking the same link on multiple pages
          find test -name "*.html" | while read -r file; do
              lynx -dump -listonly -nonumbers "${file}" | { grep --extended-regexp "^http" || test $? = 1; } >> "${temp_file}"
          done
  
      - name: Check links
        run: |
          ${RUNNER_DEBUG:+set -o xtrace}

          distinct_urls=$(sort -u "${temp_file}")
  
          echo "## âŒ Dead links found" >> "${summary_file}"
          echo "" >> "${summary_file}"
          echo "| URL | Status | Locations |" >> "${summary_file}"
          echo "| --- | -----: | --------- |" >> "${summary_file}"

          while read -r url; do
              if [[ -n "${url}" ]]; then
                echo "::debug::Checking URL '${url}'..."
  
                # Some links will probably still fail to resolve, e.g. `localhost`, "some.dummy.url" etc, so don't treat CURL exit codes as fact
                # We want to identify when a real server responds to the request

                # First try a HEAD request to avoid downloading the whole response
                status=$(curl --globoff --silent --output /dev/null --location --head --write-out "%{http_code}" "${url}" || true)
  
                if [[ "${status}" -eq 404 ]]; then
                  # But not all servers support "HEAD" (e.g. azure.microsoft.com), so try again
                  status=$(curl --globoff --silent --output /dev/null --location --write-out "%{http_code}" "${url}" || true)
                fi

                if [[ "${status}" -eq 404 ]]; then
                    # In the event of unicode control characters, grep's behaviour may be unpredictable - but lets not fail the workflow for that
                    locations=$(grep -rl "${url}" || true)
                    echo "::debug::âŒ URL '${url}' had status ${status} (found in ${locations})" 1>&2
                    echo "| ${url} | ${status} | ${locations} |" >> "${summary_file}"
                    found_error=1

                    # TODO REMOVE
                    cat "${summary_file}" >> "$GITHUB_STEP_SUMMARY"
                    exit 1
                else
                    echo "::debug::âœ… URL '${url}' had status ${status}"
                fi
              fi
          done <<< "${distinct_urls}"
          
          if [[ "${found_error}" -eq 1 ]]; then
              cat "${summary_file}" >> "$GITHUB_STEP_SUMMARY"
              exit 1
          else
              echo "âœ… No dead links found" >> "$GITHUB_STEP_SUMMARY"
              exit 0
          fi

      - name: Slack notification
        uses: 8398a7/action-slack@v3
        if: failure() && github.event_name == 'schedule'
        with:
          status: failure
          fields: workflow
          text: "ğŸ‘ Test external links failed - â›“ï¸â€ğŸ’¥ dead link(s) found â›“ï¸â€ğŸ’¥."
          channel: "#docs-notifications"
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_DOCS }}
